{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OuJk7tHj7Ffr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def generate_data():\n",
        "    \"\"\"\n",
        "    Generates a random dataset from a normal distribution.\n",
        "\n",
        "    Returns:\n",
        "        diabetes_X_train: the training dataset\n",
        "        diabetes_y_train: The output corresponding to the training set\n",
        "        diabetes_X_test: the test dataset\n",
        "        diabetes_y_test: The output corresponding to the test set\n",
        "\n",
        "    \"\"\"\n",
        "    # Load the diabetes dataset\n",
        "    diabetes_X, diabetes_y = load_diabetes(return_X_y=True)\n",
        "    \n",
        "    # Use only one feature\n",
        "    diabetes_X = diabetes_X[:, np.newaxis, 2]\n",
        "    # #Independent Feature Scaling\n",
        "    # diabetes_X = (diabetes_X - int(np.mean(diabetes_X)))/np.std(diabetes_X)\n",
        "\n",
        "    # #Dependent Feature Scaling\n",
        "    # diabetes_y = (diabetes_y - int(np.mean(diabetes_y)))/np.std(diabetes_y) \n",
        "\n",
        "    # Split the data into training/testing sets\n",
        "    diabetes_X_train = diabetes_X[:-20]\n",
        "    diabetes_X_test = diabetes_X[-20:]\n",
        "\n",
        "    # Split the targets into training/testing sets\n",
        "    diabetes_y_train = diabetes_y[:-20].reshape(-1,1)\n",
        "    diabetes_y_test = diabetes_y[-20:].reshape(-1,1)\n",
        "\n",
        "    print(f\"# Training Samples: {len(diabetes_X_train)}; # Test samples: {len(diabetes_X_test)};\")\n",
        "    return diabetes_X_train, diabetes_y_train, diabetes_X_test, diabetes_y_test\n",
        "\n",
        "\n",
        "def evaluate(model, X, y, y_predicted):\n",
        "    \"\"\" Calculates and prints evaluation metrics. \"\"\"\n",
        "    # The coefficients\n",
        "    print(f\"Slope: {model.W}; Intercept: {model.b}\")\n",
        "    # The mean squared error\n",
        "    mse = mean_squared_error(y, y_predicted)\n",
        "    print(f\"Mean squared error: {mse:.2f}\")\n",
        "    # The coefficient of determination: 1 is perfect prediction\n",
        "    r2 = r2_score(y, y_predicted)\n",
        "    print(f\"Coefficient of determination: {r2:.2f}\")\n",
        "\n",
        "    # Plot outputs\n",
        "    plt.scatter(X, y, color=\"black\")\n",
        "    plt.plot(X, y_predicted, color=\"blue\", linewidth=3)\n",
        "\n",
        "    plt.xticks(())\n",
        "    plt.yticks(())\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if r2 >= 0.4:\n",
        "        print(\"****** Success ******\")\n",
        "    else:\n",
        "        print(\"****** Failed ******\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CXkyMdGG39Kt"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "class SimpleLinearRegression:\n",
        "  \n",
        "    # import numpy as np\n",
        "\n",
        "    def __init__(self, iterations=2000, lr=0.5):\n",
        "        self.iterations = iterations # number of iterations the fit method will be called\n",
        "        self.lr = lr # The learning rate\n",
        "        self.losses = [] # A list to hold the history of the calculated losses\n",
        "        self.W, self.b = None, None # the slope and the intercept of the model\n",
        "\n",
        "    def __loss(self, y, y_hat):\n",
        "        \"\"\"\n",
        "\n",
        "        :param y: the actual output on the training set\n",
        "        :param y_hat: the predicted output on the training set\n",
        "        :return:\n",
        "            loss: the sum of squared error\n",
        "\n",
        "        \"\"\"\n",
        "        #ToDO calculate the loss. use the sum of squared error formula for simplicity\n",
        "        loss = None\n",
        "        loss = 0\n",
        "        for k in range (y.shape[0]):\n",
        "          loss += ((y[k]-y_hat[k])**2)\n",
        "        loss = loss/(y.shape[0])\n",
        "        self.losses.append(loss)\n",
        "        return loss\n",
        "\n",
        "    def __init_weights(self, X):\n",
        "        \"\"\"\n",
        "\n",
        "        :param X: The training set\n",
        "        \"\"\"\n",
        "        weights = np.random.normal(size=X.shape[1] + 1)\n",
        "        self.W = weights[:X.shape[1]].reshape(-1, X.shape[1])\n",
        "        self.b = weights[-1]\n",
        "\n",
        "    def __sgd(self, X, y, y_hat):\n",
        "        \"\"\"\n",
        "\n",
        "        :param X: The training set\n",
        "        :param y: The actual output on the training set\n",
        "        :param y_hat: The predicted output on the training set\n",
        "        :return:\n",
        "            sets updated W and b to the instance Object (self)\n",
        "        \"\"\"\n",
        "        # ToDo calculate dW & db.\n",
        "        n = X.shape[0]\n",
        "        sumW = 0.0\n",
        "        sumb = 0.0\n",
        "        for k in range(n):\n",
        "          sumW += X[k]*(y_hat[k]-y[k])\n",
        "        dW = (sumW*2)/n\n",
        "        # print(\"dW \", dW)\n",
        "        # db = None\n",
        "        for k in range(n):\n",
        "          sumb += (y_hat[k]-y[k])\n",
        "        db = (sumb*2)/n\n",
        "        #  ToDO update the self.W and self.b using the learning rate and the values for dW and db\n",
        "        self.W = self.W-self.lr*dW\n",
        "        self.b = self.b-self.lr*db\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "\n",
        "        :param X: The training set\n",
        "        :param y: The true output of the training set\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.__init_weights(X)\n",
        "        y_hat = self.predict(X)\n",
        "        loss = self.__loss(y, y_hat)\n",
        "        print(f\"Initial Loss: {loss}\")\n",
        "        for i in range(self.iterations + 1):\n",
        "            self.__sgd(X, y, y_hat)\n",
        "            y_hat = self.predict(X)\n",
        "            loss = self.__loss(y, y_hat)\n",
        "            if not i % 100:\n",
        "                print(f\"Iteration {i}, Loss: {loss}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "\n",
        "        :param X: The training dataset\n",
        "        :return:\n",
        "            y_hat: the predicted output\n",
        "        \"\"\"\n",
        "        #ToDO calculate the predicted output y_hat. remember the function of a line is defined as y = WX + b\n",
        "        import numpy as np\n",
        "        y_ht = []\n",
        "        for k in range(X.shape[0]):\n",
        "          y_ht.append(self.W*X[k] + self.b)\n",
        "        y_hat = np.array(y_ht).reshape((-1,1))\n",
        "        return y_hat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "Z-bsySHt5wJm",
        "outputId": "af301350-deeb-4e26-af52-d5f1c310ef22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# Training Samples: 422; # Test samples: 20;\n",
            "Initial Loss: [29197.36161984]\n",
            "Iteration 0, Loss: [5940.99822635]\n",
            "Iteration 100, Loss: [5216.98157592]\n",
            "Iteration 200, Loss: [4756.91548872]\n",
            "Iteration 300, Loss: [4464.57303435]\n",
            "Iteration 400, Loss: [4278.80817794]\n",
            "Iteration 500, Loss: [4160.76654094]\n",
            "Iteration 600, Loss: [4085.75866091]\n",
            "Iteration 700, Loss: [4038.09596992]\n",
            "Iteration 800, Loss: [4007.80939056]\n",
            "Iteration 900, Loss: [3988.56421428]\n",
            "Iteration 1000, Loss: [3976.33514062]\n",
            "Iteration 1100, Loss: [3968.56434963]\n",
            "Iteration 1200, Loss: [3963.62651097]\n",
            "Iteration 1300, Loss: [3960.48883159]\n",
            "Iteration 1400, Loss: [3958.49503784]\n",
            "Iteration 1500, Loss: [3957.22810995]\n",
            "Iteration 1600, Loss: [3956.42305863]\n",
            "Iteration 1700, Loss: [3955.91150021]\n",
            "Iteration 1800, Loss: [3955.58643769]\n",
            "Iteration 1900, Loss: [3955.37988133]\n",
            "Iteration 2000, Loss: [3955.24862803]\n",
            "Slope: [[928.18700531]]; Intercept: [152.92362572]\n",
            "Mean squared error: 2559.72\n",
            "Coefficient of determination: 0.47\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "****** Success ******\n"
          ]
        }
      ],
      "source": [
        "X_train, y_train, X_test, y_test = generate_data()\n",
        "model = SimpleLinearRegression()\n",
        "model.fit(X_train,y_train)\n",
        "predicted = model.predict(X_test)\n",
        "evaluate(model, X_test, y_test, predicted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dill\n",
        "dill.dump(model,open('slrm.pkl','wb'))\n",
        "saved_model=dill.load(open('slrm.pkl','rb'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.16 (default, Jan 17 2023, 16:06:28) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "bdbf1d3f5137dcb063420b308fc4ffda573f82124ce356fac28b1f566daca445"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
